
import os
# –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ HF/transformers –∫–µ—à—ñ –Ω–∞ D: (—â–æ–± –Ω—ñ—á–æ–≥–æ –Ω–µ –ø–∏—Å–∞–ª–æ—Å—å –Ω–∞ C:)
HF_BASE = os.environ.get("HF_HOME", r"D:\hf_cache")
os.environ["HF_HOME"] = HF_BASE
os.environ["TRANSFORMERS_CACHE"] = os.path.join(HF_BASE, "transformers")
os.environ["HF_DATASETS_CACHE"] = os.path.join(HF_BASE, "datasets")
os.environ["HF_MODULES_CACHE"] = os.path.join(HF_BASE, "modules")
os.environ["HF_METRICS_CACHE"] = os.path.join(HF_BASE, "metrics")
os.environ["XDG_CACHE_HOME"] = HF_BASE

from llama_cpp import Llama

# –í–∫–∞–∑—É—î–º–æ —Ä–µ–∞–ª—å–Ω–∏–π –∞–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö –¥–æ —Ç–≤–æ—î—ó –º–æ–¥–µ–ª—ñ –Ω–∞ –¥–∏—Å–∫—É D:
MODEL_PATH = r"D:\Lex\models\qwen2.5-3b-instruct-q4_k_m.gguf"

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,
    n_threads=6,
    verbose=True
)

print("üî• –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞! –ü–æ—á–∏–Ω–∞—î–º–æ –¥–æ–æ–±—É—á–µ–Ω–Ω—è...\n")

# –ß–∏—Ç–∞—î–º–æ —Ç–≤—ñ–π data.txt
DATA_PATH = r"D:\Lex\data\Data.txt"
with open(DATA_PATH, "r", encoding="utf-8") as f:
    lines = f.readlines()

# –ü—Ä–æ—Ö–æ–¥–∏–º–æ—Å—è –ø–æ –ª—ñ–Ω—ñ—è—Ö
for i, line in enumerate(lines):
    prompt = f" –ù–∞–≤—á–∏—Å—è –∑ —Ü—å–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É: {line}\n –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π 'OK' —è–∫—â–æ –∑—Ä–æ–∑—É–º—ñ–≤."
    print(f"\n==== TRAINING {i+1}/{len(lines)} ====")
    
    output = llm(
        prompt,
        max_tokens=50,
        temperature=0.1,
        stop=["</s>"]
    )

    print(" AI:", output["choices"][0]["text"].strip())

print("\n üéâüéâüéâ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. ")